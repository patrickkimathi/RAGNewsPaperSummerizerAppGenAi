# -*- coding: utf-8 -*-
"""NewssummarizerApp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d8COjs4ogZY-fNCPEB7whmeFLHp_JHEz
"""

## Import Libraries
#!pip install langchain langchain-community transformers sentence-transformers faiss-cpu pypdf langchain-text-splitters

from langchain_community.document_loaders import PyPDFLoader #for
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, pipeline

## Import Libraries
#!pip install langchain langchain-community transformers sentence-transformers faiss-cpu pypdf langchain-text-splitters langchain-huggingface

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, pipeline

# --- Load PDF ---
loader = PyPDFLoader("/content/MyGov December 2, 2025.pdf")
docs = loader.load()

# --- Split Documents into Chunks ---
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(docs)

# --- Create Embeddings and Vector Store ---
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(chunks, embeddings)
# retriever = vectorstore.as_retriever(search_kwargs={"k": 5}) # Explicitly setting k for initialization

model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
flan_pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer)

def query_rag(question):
    # Using similarity_search directly on the vectorstore as a workaround for the AttributeError
    relevant_docs = vectorstore.similarity_search(question, k=5)
    context = "\n".join([doc.page_content for doc in relevant_docs])
    prompt = f"Answer the question using only the context:\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:"
    response = flan_pipeline(
        prompt,
        max_new_tokens=200,
        temperature=0.9,      # Creativity control (lower = deterministic, higher = more diverse)
        top_k=50,             # Only sample from the top-k most likely tokens
        top_p=0.9,            # Nucleus sampling: only sample from tokens with cumulative prob <= top_p
        do_sample=True        # Enables sampling (required for temperature/top-k/top-p to work)
    )
    return response[0]['generated_text']

print(query_rag("Summarize the key points of this document in a paragraph of 200 words."))

print(query_rag("What is the purpose of the public awareness campaigns mentioned in the document?"))

print(query_rag("Summarize the key points of this document in a paragraph of 200 words."))

print(query_rag("What are some of the job opportunities mentioned in the document?"))



"""# Task
Save the FAISS vectorstore to a local directory named "faiss_index", and save the Flan-T5 model and tokenizer to a local directory named "flan_t5_model". Confirm that both directories and their contents are successfully saved.

## Save FAISS Vectorstore

### Subtask:
Save the `vectorstore` object to a local directory named 'faiss_index' using `vectorstore.save_local()`.

**Reasoning**:
The subtask requires saving the `vectorstore` object to a local directory named 'faiss_index' using the `save_local()` method.
"""

vectorstore.save_local("faiss_index")
print("Vectorstore saved successfully to 'faiss_index'.")

"""## Save Flan-T5 Model and Tokenizer

### Subtask:
Save the `model` object and the `tokenizer` object to a local directory named 'flan_t5_model' using their respective `save_pretrained()` methods.

**Reasoning**:
The subtask requires saving the `model` object to a local directory named 'flan_t5_model' using the `save_pretrained()` method.
"""

model.save_pretrained("flan_t5_model")
print("Flan-T5 model saved successfully to 'flan_t5_model'.")

"""**Reasoning**:
The subtask also requires saving the `tokenizer` object to the same local directory named 'flan_t5_model' using its `save_pretrained()` method.


"""

tokenizer.save_pretrained("flan_t5_model")
print("Flan-T5 tokenizer saved successfully to 'flan_t5_model'.")

"""**Reasoning**:
Both the FAISS vectorstore and the Flan-T5 model/tokenizer have been saved. To confirm their successful saving and content, I will list the contents of the 'faiss_index' directory using a shell command.


"""

import os

print("Contents of 'faiss_index' directory:")
print(os.listdir("faiss_index"))

"""**Reasoning**:
To confirm that the Flan-T5 model and tokenizer were successfully saved, I will list the contents of the 'flan_t5_model' directory using a shell command.


"""

import os

print("Contents of 'flan_t5_model' directory:")
print(os.listdir("flan_t5_model"))

"""## Final Task

### Subtask:
Confirm that the RAG components (FAISS index, Flan-T5 model, and tokenizer) have been successfully saved to disk.

## Summary:

### Q&A
*   **Have the RAG components (FAISS index, Flan-T5 model, and tokenizer) been successfully saved to disk?**
    Yes, all components have been successfully saved to disk. The FAISS vectorstore was saved to the `faiss_index` directory, and the Flan-T5 model and tokenizer were saved to the `flan_t5_model` directory.

### Data Analysis Key Findings
*   The FAISS vectorstore was successfully saved to a local directory named `faiss_index`. This directory contains `index.faiss` and `index.pkl` files.
*   The Flan-T5 model and tokenizer were successfully saved to a local directory named `flan_t5_model`. This directory contains files such as `generation_config.json`, `model.safetensors`, `special_tokens_map.json`, `config.json`, `tokenizer_config.json`, `spiece.model`, and `tokenizer.json`, confirming both model and tokenizer components are present.

### Insights or Next Steps
*   The saved components can now be reloaded later for inference without needing to re-train or re-generate the vectorstore, optimizing future usage.
*   The next logical step would be to load these saved components and verify that they can be used to perform RAG (Retrieval Augmented Generation) tasks successfully.

### Create `app.py` for Streamlit
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# import streamlit as st
# from langchain_community.vectorstores import FAISS
# from langchain_huggingface.embeddings import HuggingFaceEmbeddings
# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
# import os
# 
# st.title("RAG with Flan-T5 and FAISS")
# st.write("Ask a question about your document!")
# 
# # Load Embeddings Model
# @st.cache_resource
# def load_embeddings_model():
#     return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
# 
# embeddings = load_embeddings_model()
# 
# # Load FAISS Vector Store
# @st.cache_resource
# def load_vectorstore():
#     if not os.path.exists("faiss_index"):
#         st.error("FAISS index not found. Please ensure 'faiss_index' directory exists.")
#         st.stop()
#     return FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
# 
# vectorstore = load_vectorstore()
# 
# # Load Flan-T5 Model and Tokenizer
# @st.cache_resource
# def load_flan_t5():
#     model_name = "google/flan-t5-large"
#     if not os.path.exists("flan_t5_model"):
#         st.error("Flan-T5 model not found. Please ensure 'flan_t5_model' directory exists.")
#         st.stop()
#     tokenizer = AutoTokenizer.from_pretrained("flan_t5_model")
#     model = AutoModelForSeq2SeqLM.from_pretrained("flan_t5_model")
#     flan_pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer)
#     return flan_pipeline
# 
# flan_pipeline = load_flan_t5()
# 
# def query_rag(question):
#     relevant_docs = vectorstore.similarity_search(question, k=5)
#     context = "\n".join([doc.page_content for doc in relevant_docs])
#     prompt = f"Answer the question using only the context:\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:"
#     response = flan_pipeline(
#         prompt,
#         max_new_tokens=200,
#         temperature=0.9,
#         top_k=50,
#         top_p=0.9,
#         do_sample=True
#     )
#     return response[0]['generated_text']
# 
# # Streamlit UI
# question = st.text_input("Your Question:")
# 
# if st.button("Get Answer"):
#     if question:
#         with st.spinner("Retrieving and Generating Answer..."):
#             answer = query_rag(question)
#             st.write("### Answer:")
#             st.write(answer)
#     else:
#         st.warning("Please enter a question.")
#

"""# Task
Install Streamlit, run the `app.py` application, and confirm that the Streamlit application is running and accessible via the provided public URL.

## Install Streamlit

### Subtask:
Ensure that Streamlit is installed in the environment.

**Reasoning**:
To ensure Streamlit is installed, I will use `pip install streamlit`.
"""

#import sys
#!{sys.executable} -m pip install streamlit

"""**Reasoning**:
Streamlit has been installed. Now, I will run the `app.py` script to start the Streamlit application.


"""

import subprocess

# Start Streamlit in the background
# The --server.port 8501 is the default, but explicitly setting it can be good practice.
# The --server.enableCORS false and --server.enableXsrfProtection false are often needed in Colab for external access.
# Using nohup and & to run in background so the cell doesn't block.
# The output is redirected to streamlit.log file.
# The output should show a public URL if it starts successfully.
command = ["nohup", "streamlit", "run", "app.py", "--server.port", "8501", "--server.enableCORS", "false", "--server.enableXsrfProtection", "false", "&"]
process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

print("Streamlit app is attempting to start in the background. Please wait a few moments and check the output for a public URL.")
# You might need to check the nohup.out file or a specific log file for the public URL generated by Streamlit
# In Colab, Streamlit usually prints the public URL to stdout, so we might capture it after a short delay.
# However, since it's running in nohup, it won't appear directly in this cell's output.
# A separate command to check `nohup.out` or `/tmp/streamlit_output.log` might be needed.
# For now, we'll assume it starts and provide instructions to find the URL.

"""**Reasoning**:
Since Streamlit was started with `nohup`, its output, including the public URL, is redirected to `nohup.out`. I will wait a few seconds for Streamlit to fully initialize and then read the `nohup.out` file to extract the public URL and display it to the user for confirmation.


"""

import time

# Give Streamlit some time to start up and generate the URL
time.sleep(10) # Wait for 10 seconds

# Read the nohup.out file to find the public URL
try:
    with open("nohup.out", "r") as f:
        nohup_output = f.read()

    # Search for the public URL pattern
    # Streamlit's public URL typically looks like 'https://<some-hash>.ngrok.io'
    # or a similar pattern from other tunnel services.
    # We'll look for 'External URL:' followed by a URL.
    import re
    match = re.search(r'External URL: (https?://[\w\d.-]+\.[a-zA-Z]{2,6}(?:/[^\s]*)?)', nohup_output)

    if match:
        public_url = match.group(1)
        print(f"\nStreamlit application is running. Access it at: {public_url}")
    else:
        print("\nCould not find the public URL in nohup.out. Please check the 'nohup.out' file manually for the URL.")
        print("Content of nohup.out (last 500 chars):\n" + nohup_output[-500:])
except FileNotFoundError:
    print("\nnohup.out file not found. Streamlit might not have started correctly or the file is in a different location.")
except Exception as e:
    print(f"\nAn error occurred while reading nohup.out: {e}")

"""**Reasoning**:
Since the previous attempt to read `nohup.out` failed because the file was not found, I need to ensure that the Streamlit application is started correctly and its output is captured to a file that can be reliably accessed. I will first try to terminate any lingering Streamlit processes to avoid port conflicts, then restart the Streamlit application, explicitly redirecting its output to a known log file (`streamlit_output.log`).


"""

import subprocess
import os
import time

# Kill any processes potentially using port 8501 or named 'streamlit'
# This is a precaution to ensure a clean start
#!kill $(lsof -t -i:8501) 2>/dev/null || true
#!pkill -f streamlit 2>/dev/null || true

# Ensure the previous log file is removed for a fresh start
if os.path.exists("streamlit_output.log"):
    os.remove("streamlit_output.log")

# Start Streamlit in the background and redirect output to a specific log file
# The --server.port 8501 is the default, but explicitly setting it can be good practice.
# The --server.enableCORS false and --server.enableXsrfProtection false are often needed in Colab for external access.
# Using '&' to run in background so the cell doesn't block.
# Output is redirected to 'streamlit_output.log'
command_str = "streamlit run app.py --server.port 8501 --server.enableCORS false --server.enableXsrfProtection false > streamlit_output.log 2>&1 &"
subprocess.Popen(command_str, shell=True)

print("Attempting to start Streamlit app in the background. Please wait a few moments and then try retrieving the public URL.")

# Give Streamlit some time to start up and write to the log file
time.sleep(10)

# Now, try to read the log file to find the public URL
try:
    with open("streamlit_output.log", "r") as f:
        log_output = f.read()

    import re
    match = re.search(r'External URL: (https?://[\w\d.-]+\.[a-zA-Z]{2,6}(?:/[^\s]*)?)', log_output)

    if match:
        public_url = match.group(1)
        print(f"\nStreamlit application is running. Access it at: {public_url}")
    else:
        print("\nCould not find the public URL in streamlit_output.log. Please check the log file manually for the URL.")
        print("Content of streamlit_output.log (last 500 chars):\n" + log_output[-500:])
except FileNotFoundError:
    print("\nstreamlit_output.log file not found. Streamlit might not have started correctly or the redirection failed.")
except Exception as e:
    print(f"\nAn error occurred while reading streamlit_output.log: {e}")
