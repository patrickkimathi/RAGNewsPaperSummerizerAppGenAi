# -*- coding: utf-8 -*-
"""streamlit_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10A3kiVH6k2uSQtY_cpwqbHNorJjHAPy2
"""



# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# import streamlit as st
# from langchain_community.vectorstores import FAISS
# from langchain_huggingface.embeddings import HuggingFaceEmbeddings
# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
# import os
# 
# st.title("RAG with Flan-T5 and FAISS")
# st.write("Ask a question about your document!")
# 
# # Load Embeddings Model
# @st.cache_resource
# def load_embeddings_model():
#     return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
# 
# embeddings = load_embeddings_model()
# 
# # Load FAISS Vector Store
# @st.cache_resource
# def load_vectorstore():
#     if not os.path.exists("faiss_index"):
#         st.error("FAISS index not found. Please ensure 'faiss_index' directory exists.")
#         st.stop()
#     return FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
# 
# vectorstore = load_vectorstore()
# 
# # Load Flan-T5 Model and Tokenizer
# @st.cache_resource
# def load_flan_t5():
#     model_name = "google/flan-t5-large"
#     if not os.path.exists("flan_t5_model"):
#         st.error("Flan-T5 model not found. Please ensure 'flan_t5_model' directory exists.")
#         st.stop()
#     tokenizer = AutoTokenizer.from_pretrained("flan_t5_model")
#     model = AutoModelForSeq2SeqLM.from_pretrained("flan_t5_model")
#     flan_pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer)
#     return flan_pipeline
# 
# flan_pipeline = load_flan_t5()
# 
# def query_rag(question):
#     relevant_docs = vectorstore.similarity_search(question, k=5)
#     context = "\n".join([doc.page_content for doc in relevant_docs])
#     prompt = f"Answer the question using only the context:\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:"
#     response = flan_pipeline(
#         prompt,
#         max_new_tokens=200,
#         temperature=0.9,
#         top_k=50,
#         top_p=0.9,
#         do_sample=True
#     )
#     return response[0]['generated_text']
# 
# # Streamlit UI
# question = st.text_input("Your Question:")
# 
# if st.button("Get Answer"):
#     if question:
#         with st.spinner("Retrieving and Generating Answer..."):
#             answer = query_rag(question)
#             st.write("### Answer:")
#             st.write(answer)
#     else:
#         st.warning("Please enter a question.")
#